{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sumber : [channel utube analytics exellence](https://www.youtube.com/watch?v=QLGrLFOzMRw&list=PL0hSJrxggIQr6wA8buIn1Yxu810ugGed-)\n",
    "[data](https://github.com/jleetutorial/python-spark-tutorial/tree/master/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. spark job => menghitung kata (word count)\n",
    "kita akan melakukan job dg pyspark yaitu menghitung kata dari suatu kalimat dalam dataset.\n",
    "\n",
    "lalu kita akan load datanya memakai rdd. NOTE : ada 3 cara dalam spark untuk load data yaitu rdd (lewat ram), dataframe (sprti pandas), dan dataset (khusus scala & java)\n",
    "\n",
    "[untuk load data dlm dataframe & raw data bntk file bisa dilakukan dg](https://www.geeksforgeeks.org/read-text-file-into-pyspark-dataframe/), [atau ini](https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/)\n",
    "\n",
    "* spark.read.text()\n",
    "* spark.read.csv()\n",
    "* spark.read.format().load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/19 12:33:29 WARN Utils: Your hostname, sufyan-X550IK resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlp1s0)\n",
      "22/09/19 12:33:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kafka/streampyspark/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/19 12:33:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 24\n",
      "count 11\n",
      "from 2\n",
      "Wikipedia 1\n",
      "the 38\n",
      "free 1\n",
      "encyclopedia 1\n",
      "is 19\n",
      "number 3\n",
      "of 25\n",
      "words 21\n",
      "in 11\n",
      "a 28\n",
      "document 2\n",
      "or 11\n",
      "passage 1\n",
      "text 8\n",
      "Word 3\n",
      "counting 6\n",
      "may 8\n",
      "be 8\n",
      "needed 1\n",
      "when 2\n",
      "required 1\n",
      "to 18\n",
      "stay 1\n",
      "within 1\n",
      "certain 2\n",
      "numbers 1\n",
      "This 2\n",
      "particularly 1\n",
      "case 1\n",
      "academia 1\n",
      "legal 1\n",
      "proceedings 1\n",
      "journalism 1\n",
      "and 23\n",
      "advertising 1\n",
      "commonly 1\n",
      "used 4\n",
      "by 5\n",
      "translators 1\n",
      "determine 1\n",
      "price 1\n",
      "for 10\n",
      "translation 1\n",
      "job 1\n",
      "counts 3\n",
      "also 5\n",
      "calculate 1\n",
      "measures 1\n",
      "readability 1\n",
      "measure 2\n",
      "typing 1\n",
      "reading 1\n",
      "speeds 1\n",
      "usually 3\n",
      "per 3\n",
      "minute 1\n",
      "When 1\n",
      "converting 1\n",
      "character 2\n",
      "five 1\n",
      "six 1\n",
      "characters 2\n",
      "generally 2\n",
      "Contents 1\n",
      "Details 2\n",
      "variations 2\n",
      "definition 3\n",
      "Software 2\n",
      "In 4\n",
      "fiction 4\n",
      "non 2\n",
      "See 1\n",
      "References 1\n",
      "Sources 1\n",
      "External 1\n",
      "links 1\n",
      "section 2\n",
      "does 1\n",
      "not 2\n",
      "cite 1\n",
      "any 2\n",
      "references 1\n",
      "sources 2\n",
      "Please 1\n",
      "help 1\n",
      "improve 1\n",
      "this 2\n",
      "adding 1\n",
      "citations 1\n",
      "reliable 1\n",
      "Unsourced 1\n",
      "material 1\n",
      "challenged 1\n",
      "removed 1\n",
      "Variations 1\n",
      "operational 2\n",
      "definitions 3\n",
      "how 3\n",
      "can 5\n",
      "occur 1\n",
      "namely 2\n",
      "what 1\n",
      "as 9\n",
      "which 4\n",
      "don't 2\n",
      "toward 2\n",
      "total 2\n",
      "However 3\n",
      "especially 1\n",
      "since 1\n",
      "advent 1\n",
      "widespread 1\n",
      "processing 4\n",
      "there 1\n",
      "broad 1\n",
      "consensus 3\n",
      "on 7\n",
      "these 2\n",
      "hence 1\n",
      "bottom 1\n",
      "line 3\n",
      "integer 1\n",
      "result 1\n",
      "The 5\n",
      "accept 1\n",
      "segmentation 4\n",
      "rules 5\n",
      "found 1\n",
      "most 3\n",
      "software 2\n",
      "including 1\n",
      "boundaries 1\n",
      "are 4\n",
      "determined 1\n",
      "depends 1\n",
      "dividers 1\n",
      "defined 1\n",
      "first 1\n",
      "trait 1\n",
      "that 5\n",
      "space 3\n",
      "various 1\n",
      "whitespace 1\n",
      "such 7\n",
      "regular 1\n",
      "an 4\n",
      "em 1\n",
      "tab 1\n",
      "divider 1\n",
      "Usually 1\n",
      "hyphen 1\n",
      "slash 1\n",
      "too 1\n",
      "Different 1\n",
      "programs 2\n",
      "give 2\n",
      "varying 2\n",
      "results 2\n",
      "depending 2\n",
      "rule 4\n",
      "details 2\n",
      "whether 1\n",
      "outside 1\n",
      "main 1\n",
      "footnotes 2\n",
      "endnotes 2\n",
      "hidden 2\n",
      "text) 1\n",
      "counted 2\n",
      "But 2\n",
      "behavior 1\n",
      "major 1\n",
      "applications 2\n",
      "broadly 1\n",
      "similar 1\n",
      "during 1\n",
      "era 2\n",
      "school 1\n",
      "assignments 1\n",
      "were 3\n",
      "done 1\n",
      "handwriting 1\n",
      "with 2\n",
      "typewriters 1\n",
      "often 3\n",
      "differed 1\n",
      "todays 1\n",
      "Most 2\n",
      "importantly 1\n",
      "many 2\n",
      "students 2\n",
      "drilled 1\n",
      "articles 1\n",
      "but 2\n",
      "sometimes 1\n",
      "others 1\n",
      "conjunctions 1\n",
      "example 2\n",
      "some 1\n",
      "prepositions 1\n",
      "Hyphenated 1\n",
      "permanent 1\n",
      "compounds 1\n",
      "follow 1\n",
      "up 2\n",
      "noun 1\n",
      "long 1\n",
      "term 1\n",
      "adjective 1\n",
      "one 1\n",
      "To 1\n",
      "save 1\n",
      "time 1\n",
      "effort 1\n",
      "thumb 1\n",
      "average 1\n",
      "was 1\n",
      "10 1\n",
      "These 1\n",
      "have 1\n",
      "fallen 1\n",
      "wayside 1\n",
      "feature 1\n",
      "follows 1\n",
      "mentioned 1\n",
      "earlier 2\n",
      "now 1\n",
      "standard 1\n",
      "arbiter 1\n",
      "because 2\n",
      "it 2\n",
      "largely 1\n",
      "consistent 1\n",
      "across 1\n",
      "documents 1\n",
      "fast 1\n",
      "effortless 1\n",
      "costless 1\n",
      "already 1\n",
      "included 1\n",
      "application 1\n",
      "As 2\n",
      "sections 1\n",
      "abstracts 1\n",
      "reference 1\n",
      "lists 2\n",
      "bibliographies 1\n",
      "tables 1\n",
      "figure 1\n",
      "captions 1\n",
      "person 1\n",
      "charge 1\n",
      "teacher 1\n",
      "client 1\n",
      "define 1\n",
      "their 1\n",
      "choice 1\n",
      "users 1\n",
      "workers 1\n",
      "simply 1\n",
      "select 1\n",
      "exclude 1\n",
      "elements 1\n",
      "accordingly 1\n",
      "watch 1\n",
      "automatically 1\n",
      "update 1\n",
      "Modern 1\n",
      "web 1\n",
      "browsers 1\n",
      "support 1\n",
      "via 2\n",
      "extensions 1\n",
      "JavaScript 1\n",
      "bookmarklet 1\n",
      "script 1\n",
      "hosted 1\n",
      "website 1\n",
      "processors 1\n",
      "Unix 1\n",
      "like 1\n",
      "systems 1\n",
      "include 1\n",
      "program 1\n",
      "wc 1\n",
      "specifically 1\n",
      "explained 1\n",
      "different 1\n",
      "exact 1\n",
      "strict 1\n",
      "requirement 1\n",
      "thus 1\n",
      "variation 1\n",
      "acceptable 2\n",
      "Novelist 1\n",
      "Jane 1\n",
      "Smiley 2\n",
      "suggests 1\n",
      "length 5\n",
      "important 1\n",
      "quality 1\n",
      "novel 4\n",
      "novels 3\n",
      "vary 2\n",
      "tremendously 1\n",
      "typically 1\n",
      "being 1\n",
      "between 2\n",
      "while 3\n",
      "National 1\n",
      "Novel 2\n",
      "Writing 1\n",
      "Month 1\n",
      "requires 1\n",
      "its 3\n",
      "at 3\n",
      "least 1\n",
      "There 1\n",
      "no 1\n",
      "firm 1\n",
      "boundary 1\n",
      "novella 1\n",
      "arbitrary 1\n",
      "literary 1\n",
      "work 1\n",
      "difficult 1\n",
      "categorise 1\n",
      "large 1\n",
      "extent 1\n",
      "writer 1\n",
      "lengths 2\n",
      "subgenre 1\n",
      "chapter 1\n",
      "books 1\n",
      "children 1\n",
      "start 1\n",
      "about 1\n",
      "typical 1\n",
      "mystery 1\n",
      "might 1\n",
      "range 1\n",
      "thriller 1\n",
      "could 1\n",
      "over 2\n",
      "Science 1\n",
      "Fiction 1\n",
      "Fantasy 1\n",
      "Writers 1\n",
      "America 1\n",
      "specifies 1\n",
      "each 1\n",
      "category 1\n",
      "Nebula 1\n",
      "award 1\n",
      "categories 1\n",
      "Classification\tWord 1\n",
      "Novella 1\n",
      "Novelette 1\n",
      "Short 1\n",
      "story 1\n",
      "under 1\n",
      "academic 1\n",
      "dissertation 1\n",
      "varies 1\n",
      "greatly 1\n",
      "dependent 1\n",
      "predominantly 1\n",
      "subject 1\n",
      "Numerous 1\n",
      "American 1\n",
      "universities 1\n",
      "limit 2\n",
      "Ph.D. 1\n",
      "dissertations 1\n",
      "barring 1\n",
      "special 1\n",
      "permission 1\n",
      "exceeding 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[3]\", \"word count\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "line = sc.textFile(\"data/wordcount.txt\")\n",
    "\n",
    "words = line.flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [perbedaan spark context vs session](https://sparkbyexamples.com/spark/sparksession-vs-sparkcontext/)\n",
    "spark context untuk load data dengan cara rdd & berjln pd versi spark lama, sedangkan session ada di versi baru selain dpt rdd juga bisa untuk dataframe & dataset\n",
    "\n",
    "semua operasi yg dpt dilakukan oleh context maka session dpt melakukannya tp tdk berlaku sebaliknya \n",
    "\n",
    "session itu versi terbaru dr context\n",
    "\n",
    "context dan session termasuk kedlm core & mrpkn gerbang (endpoint) pertama untuk menggunakan spark\n",
    "\n",
    "appName => nama app di spark nya (ibaratnya nama container di docker), kasus diatas appName = word count\n",
    "\n",
    "master => spark akan run di local dg penggunaan cpu 3 core (worker nya 3), local[3]. untuk lbh jls [tentang master](https://spark.apache.org/docs/latest/submitting-applications.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 : RDD\n",
    "dataset akan didistribusikan kedlm RDD lalu dilakukan parallelize untuk melakukan transformasi atau operasi pd data (prosesnya dilakukan scra paralel shingga lbh cpt)\n",
    "\n",
    "## operasi pd RDD\n",
    "### transformasi \n",
    "artinya menerapkan (apply) suatu fungsi pada suatu data melalui RDD dan akan di hslkan RDD yg baru.\n",
    "\n",
    "salah satu contoh transformasi data adlh filtering. misal kita filter hanya kata the saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineWithThe = line.filter(lambda lin: \"the\" in lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action\n",
    "compute suatu hasil berdasarkan pd RDD (melakukan suatu action tertentu pd data di rdd, action disini tdk mengubah data karena mengubah data ada di transformasi bkn action)\n",
    "\n",
    "contoh action adlh first untuk melihat element pertama pada suatu rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word count from Wikipedia the free encyclopedia'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineWithThe.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## workflow spark\n",
    "1. mengenerate initial rdd dr data eksternal (data luar yg spark read/load)\n",
    "2. apply/terapkan transformasi sprti map, filter, dll pd rdd\n",
    "3. menjlnkan action. misal menghitung hasil dari suatu komputasi berdasarkan RDD dan\n",
    "memberikan return/kembalian kepada program driver atau simpan ke\n",
    "penyimpanan eksternal\n",
    "\n",
    "sumber lainnya\n",
    "\n",
    "http://malifauzi.lecture.ub.ac.id/files/2019/02/Spark.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 membuat rdd\n",
    "we can create new rdd with data yg kita punya di pass(diberikan/passing) ke method spark parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/19 14:49:15 WARN Utils: Your hostname, sufyan-X550IK resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp1s0)\n",
      "22/09/19 14:49:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kafka/streampyspark/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/19 14:49:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[3]\", \"word count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputInt = list(range(1,6))\n",
    "rddInt = sc.parallelize(inputInt)\n",
    "rddInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semua elemen pd data inputInt akan dicopy kedlm bntk rdd lalu akan dilakukan operasi (transformasi/action) melalui parallel\n",
    "\n",
    "### membuat rdd\n",
    "misal data kita dlm bntk file txt, akan kita load melalui rdd menggunakan context method nya textFile sprti contoh diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[3]\", \"word count\")\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# create rdd dari data external berupa txt\n",
    "line = sc.textFile(\"data/wordcount.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data external ini (production) biasanya adlh hdfs, amazon s3 bkn txt sprti contoh. selain itu data (sumber data yg diintegrasikan dg spark) dpt diambil dari jdbc(konektor ke db di java, dipython like alchemySql), elasticsearch, mysql, dll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 transformasi (map & filter)\n",
    "sblmnya dijlskan bahwa rdd dpt melakukan operasi transformasi & action yg akan mengembalikan RDD baru (return new rdd).\n",
    "\n",
    "transformasi yg paling sering dilakukan adlh map & filter.\n",
    "\n",
    "## filter()\n",
    "paramnya berupa func & akan return rdd baru yg terbentuk dr hasil penyeleksian elemen yg di pass kedlm filter func.\n",
    "\n",
    "filter func ini bisa menghapus elemen/row yg tdk sesuai dg standar/aturan kita \n",
    "\n",
    "## map()\n",
    "paramnya berupa func & akan passes(memberikan/passing) elemen dlm input rdd melalui suatu func, hasilnya akan ada data baru dari func tsb as hasil dr rdd\n",
    "\n",
    "misal, bisa digunakan untuk make http request untuk setiap in our input rdd atau bisa juga menghitung akar pangkat dari setiap angka dlm data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import requests\n",
    "\n",
    "def makeHttpRequest(link):\n",
    "    requests.get(link)\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"word count2\").getOrCreate()\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# make http request untuk setiap input rdd\n",
    "URLs = sc.textFile(\"data/url.txt\")\n",
    "\n",
    "URLs.map(makeHttpRequest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return value pd map func tdk bersifat wajib dan tipe datanya tdk mesti sama dg input nya. ex data inputnya string maka dg map func return value-nya bisa berupa int sbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/19 16:28:59 WARN Utils: Your hostname, sufyan-X550IK resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp1s0)\n",
      "22/09/19 16:28:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kafka/streampyspark/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/19 16:29:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[3]\", \"word count\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "lines = sc.textFile(\"data/wordcount.txt\")\n",
    "lengths = lines.map(lambda line: len(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contoh kasus kita akan doing something with airports data yg berformat csv\n",
    "\n",
    "kita hanya akan menyeleksi hanya yg berlokasi di usa saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import sys\n",
    "from commons.Utils import Utils\n",
    "\n",
    "# karena dir commons itu ada root maka gunakan code ini agar dpt mengimport\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "def splitComma(line: str):\n",
    "    splits = Utils.COMMA_DELIMITER.split(line)\n",
    "    return \"{}, {}\".format(splits[1], splits[2])\n",
    "\n",
    "# set config untuk job pd spark melalui config \n",
    "# master (dicontext/session) atau setMaster => job akan run di local. 2 core dari cpu komputer kita(karena local jd komputer kita) atau 2 worker thread di komputer kita\n",
    "configuration = SparkConf().setAppName(\"airport_in_usa\").setMaster(\"local[2]\")\n",
    "\n",
    "# context as garbang utama agar dpt konek ke spark cluster\n",
    "sc = SparkContext(conf = configuration)\n",
    "\n",
    "# load data as rdd\n",
    "airports = sc.textFile(\"data/airports.txt\")\n",
    "\n",
    "# transformasi filter dg func lambda. seleksi hanya airport yg berlokasi di usa. param filter adlh func berupa lambda\n",
    "airportsInUSA =  airports.filter(lambda line : Utils.COMMA_DELIMITER.split(line)[3] == \"\\\"United States\\\"\")\n",
    "\n",
    "# transformasi map dg func splitComma diatas\n",
    "airportsNameAndCityNames = airportsInUSA.map(splitComma)\n",
    "\n",
    "# save result data transformation into file\n",
    "airportsNameAndCityNames.saveAsTextFile(\"result_data/airports_in_usa.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if code diatas disimpan dlm file .py runnya adlh\n",
    "!spark-submit namafilenya.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8\n",
    "next case, when latitude > 40 & output is file contain nama airport, latitude.\n",
    "\n",
    "caranya mudah, dari case sblmnya (airport di usa) kita hanya ubah func pada transformasi filter saja dg latitude > 40 lalu ubah outputnya dg nama airport & latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import sys\n",
    "from commons.Utils import Utils\n",
    "\n",
    "# karena dir commons itu ada root maka gunakan code ini agar dpt mengimport\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "def splitComma(line: str):\n",
    "    splits = Utils.COMMA_DELIMITER.split(line)\n",
    "    return \"{}, {}\".format(splits[1], splits[6])\n",
    "\n",
    "# set config untuk job pd spark melalui config \n",
    "# master (dicontext/session) atau setMaster => job akan run di local. 2 core dari cpu komputer kita(karena local jd komputer kita) atau 2 worker thread di komputer kita\n",
    "configuration = SparkConf().setAppName(\"airport latitude more 40\").setMaster(\"local[2]\")\n",
    "\n",
    "# context as garbang utama agar dpt konek ke spark cluster\n",
    "sc = SparkContext(conf = configuration)\n",
    "\n",
    "# load data as rdd\n",
    "airports = sc.textFile(\"data/airports.txt\")\n",
    "\n",
    "# transformasi filter dg func lambda. seleksi hanya nama airport & latitude dg syarat latitude > 40. param filter adlh func berupa lambda\n",
    "# latitude ada dikolom ke 6\n",
    "airportsMore40lat =  airports.filter(lambda line : float(Utils.COMMA_DELIMITER.split(line)[6]) > 40 )\n",
    "\n",
    "# transformasi map dg func splitComma diatas\n",
    "airportsNameAndLatitude = airportsMore40lat.map(splitComma)\n",
    "\n",
    "# save result data transformation into file\n",
    "airportsNameAndLatitude.saveAsTextFile(\"result_data/airports_latitude_more_40.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if code diatas disimpan dlm file .py runnya adlh\n",
    "!spark-submit namafilenya.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 transformasi : flatMap\n",
    "salah satu yg paling popular juga.\n",
    "\n",
    "terkadang kita ingin produce multiple element for each input element maka kita gunakan flatMap\n",
    "\n",
    "dg flatMap result akan di flatten terlbh dahulu, hal ini berbeda dg map (lihat code menit 0:28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contoh popular dr flatMap adlh saat split input string jd 2 kata yaitu split lines by space. lebih jelas lihat menit 0:54-1:52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 24\n",
      "count 11\n",
      "from 2\n",
      "Wikipedia 1\n",
      "the 38\n",
      "free 1\n",
      "encyclopedia 1\n",
      "is 19\n",
      "number 3\n",
      "of 25\n",
      "words 21\n",
      "in 11\n",
      "a 28\n",
      "document 2\n",
      "or 11\n",
      "passage 1\n",
      "text 8\n",
      "Word 3\n",
      "counting 6\n",
      "may 8\n",
      "be 8\n",
      "needed 1\n",
      "when 2\n",
      "required 1\n",
      "to 18\n",
      "stay 1\n",
      "within 1\n",
      "certain 2\n",
      "numbers 1\n",
      "This 2\n",
      "particularly 1\n",
      "case 1\n",
      "academia 1\n",
      "legal 1\n",
      "proceedings 1\n",
      "journalism 1\n",
      "and 23\n",
      "advertising 1\n",
      "commonly 1\n",
      "used 4\n",
      "by 5\n",
      "translators 1\n",
      "determine 1\n",
      "price 1\n",
      "for 10\n",
      "translation 1\n",
      "job 1\n",
      "counts 3\n",
      "also 5\n",
      "calculate 1\n",
      "measures 1\n",
      "readability 1\n",
      "measure 2\n",
      "typing 1\n",
      "reading 1\n",
      "speeds 1\n",
      "usually 3\n",
      "per 3\n",
      "minute 1\n",
      "When 1\n",
      "converting 1\n",
      "character 2\n",
      "five 1\n",
      "six 1\n",
      "characters 2\n",
      "generally 2\n",
      "Contents 1\n",
      "Details 2\n",
      "variations 2\n",
      "definition 3\n",
      "Software 2\n",
      "In 4\n",
      "fiction 4\n",
      "non 2\n",
      "See 1\n",
      "References 1\n",
      "Sources 1\n",
      "External 1\n",
      "links 1\n",
      "section 2\n",
      "does 1\n",
      "not 2\n",
      "cite 1\n",
      "any 2\n",
      "references 1\n",
      "sources 2\n",
      "Please 1\n",
      "help 1\n",
      "improve 1\n",
      "this 2\n",
      "adding 1\n",
      "citations 1\n",
      "reliable 1\n",
      "Unsourced 1\n",
      "material 1\n",
      "challenged 1\n",
      "removed 1\n",
      "Variations 1\n",
      "operational 2\n",
      "definitions 3\n",
      "how 3\n",
      "can 5\n",
      "occur 1\n",
      "namely 2\n",
      "what 1\n",
      "as 9\n",
      "which 4\n",
      "don't 2\n",
      "toward 2\n",
      "total 2\n",
      "However 3\n",
      "especially 1\n",
      "since 1\n",
      "advent 1\n",
      "widespread 1\n",
      "processing 4\n",
      "there 1\n",
      "broad 1\n",
      "consensus 3\n",
      "on 7\n",
      "these 2\n",
      "hence 1\n",
      "bottom 1\n",
      "line 3\n",
      "integer 1\n",
      "result 1\n",
      "The 5\n",
      "accept 1\n",
      "segmentation 4\n",
      "rules 5\n",
      "found 1\n",
      "most 3\n",
      "software 2\n",
      "including 1\n",
      "boundaries 1\n",
      "are 4\n",
      "determined 1\n",
      "depends 1\n",
      "dividers 1\n",
      "defined 1\n",
      "first 1\n",
      "trait 1\n",
      "that 5\n",
      "space 3\n",
      "various 1\n",
      "whitespace 1\n",
      "such 7\n",
      "regular 1\n",
      "an 4\n",
      "em 1\n",
      "tab 1\n",
      "divider 1\n",
      "Usually 1\n",
      "hyphen 1\n",
      "slash 1\n",
      "too 1\n",
      "Different 1\n",
      "programs 2\n",
      "give 2\n",
      "varying 2\n",
      "results 2\n",
      "depending 2\n",
      "rule 4\n",
      "details 2\n",
      "whether 1\n",
      "outside 1\n",
      "main 1\n",
      "footnotes 2\n",
      "endnotes 2\n",
      "hidden 2\n",
      "text) 1\n",
      "counted 2\n",
      "But 2\n",
      "behavior 1\n",
      "major 1\n",
      "applications 2\n",
      "broadly 1\n",
      "similar 1\n",
      "during 1\n",
      "era 2\n",
      "school 1\n",
      "assignments 1\n",
      "were 3\n",
      "done 1\n",
      "handwriting 1\n",
      "with 2\n",
      "typewriters 1\n",
      "often 3\n",
      "differed 1\n",
      "todays 1\n",
      "Most 2\n",
      "importantly 1\n",
      "many 2\n",
      "students 2\n",
      "drilled 1\n",
      "articles 1\n",
      "but 2\n",
      "sometimes 1\n",
      "others 1\n",
      "conjunctions 1\n",
      "example 2\n",
      "some 1\n",
      "prepositions 1\n",
      "Hyphenated 1\n",
      "permanent 1\n",
      "compounds 1\n",
      "follow 1\n",
      "up 2\n",
      "noun 1\n",
      "long 1\n",
      "term 1\n",
      "adjective 1\n",
      "one 1\n",
      "To 1\n",
      "save 1\n",
      "time 1\n",
      "effort 1\n",
      "thumb 1\n",
      "average 1\n",
      "was 1\n",
      "10 1\n",
      "These 1\n",
      "have 1\n",
      "fallen 1\n",
      "wayside 1\n",
      "feature 1\n",
      "follows 1\n",
      "mentioned 1\n",
      "earlier 2\n",
      "now 1\n",
      "standard 1\n",
      "arbiter 1\n",
      "because 2\n",
      "it 2\n",
      "largely 1\n",
      "consistent 1\n",
      "across 1\n",
      "documents 1\n",
      "fast 1\n",
      "effortless 1\n",
      "costless 1\n",
      "already 1\n",
      "included 1\n",
      "application 1\n",
      "As 2\n",
      "sections 1\n",
      "abstracts 1\n",
      "reference 1\n",
      "lists 2\n",
      "bibliographies 1\n",
      "tables 1\n",
      "figure 1\n",
      "captions 1\n",
      "person 1\n",
      "charge 1\n",
      "teacher 1\n",
      "client 1\n",
      "define 1\n",
      "their 1\n",
      "choice 1\n",
      "users 1\n",
      "workers 1\n",
      "simply 1\n",
      "select 1\n",
      "exclude 1\n",
      "elements 1\n",
      "accordingly 1\n",
      "watch 1\n",
      "automatically 1\n",
      "update 1\n",
      "Modern 1\n",
      "web 1\n",
      "browsers 1\n",
      "support 1\n",
      "via 2\n",
      "extensions 1\n",
      "JavaScript 1\n",
      "bookmarklet 1\n",
      "script 1\n",
      "hosted 1\n",
      "website 1\n",
      "processors 1\n",
      "Unix 1\n",
      "like 1\n",
      "systems 1\n",
      "include 1\n",
      "program 1\n",
      "wc 1\n",
      "specifically 1\n",
      "explained 1\n",
      "different 1\n",
      "exact 1\n",
      "strict 1\n",
      "requirement 1\n",
      "thus 1\n",
      "variation 1\n",
      "acceptable 2\n",
      "Novelist 1\n",
      "Jane 1\n",
      "Smiley 2\n",
      "suggests 1\n",
      "length 5\n",
      "important 1\n",
      "quality 1\n",
      "novel 4\n",
      "novels 3\n",
      "vary 2\n",
      "tremendously 1\n",
      "typically 1\n",
      "being 1\n",
      "between 2\n",
      "while 3\n",
      "National 1\n",
      "Novel 2\n",
      "Writing 1\n",
      "Month 1\n",
      "requires 1\n",
      "its 3\n",
      "at 3\n",
      "least 1\n",
      "There 1\n",
      "no 1\n",
      "firm 1\n",
      "boundary 1\n",
      "novella 1\n",
      "arbitrary 1\n",
      "literary 1\n",
      "work 1\n",
      "difficult 1\n",
      "categorise 1\n",
      "large 1\n",
      "extent 1\n",
      "writer 1\n",
      "lengths 2\n",
      "subgenre 1\n",
      "chapter 1\n",
      "books 1\n",
      "children 1\n",
      "start 1\n",
      "about 1\n",
      "typical 1\n",
      "mystery 1\n",
      "might 1\n",
      "range 1\n",
      "thriller 1\n",
      "could 1\n",
      "over 2\n",
      "Science 1\n",
      "Fiction 1\n",
      "Fantasy 1\n",
      "Writers 1\n",
      "America 1\n",
      "specifies 1\n",
      "each 1\n",
      "category 1\n",
      "Nebula 1\n",
      "award 1\n",
      "categories 1\n",
      "Classification\tWord 1\n",
      "Novella 1\n",
      "Novelette 1\n",
      "Short 1\n",
      "story 1\n",
      "under 1\n",
      "academic 1\n",
      "dissertation 1\n",
      "varies 1\n",
      "greatly 1\n",
      "dependent 1\n",
      "predominantly 1\n",
      "subject 1\n",
      "Numerous 1\n",
      "American 1\n",
      "universities 1\n",
      "limit 2\n",
      "Ph.D. 1\n",
      "dissertations 1\n",
      "barring 1\n",
      "special 1\n",
      "permission 1\n",
      "exceeding 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "configuration = SparkConf().setAppName(\"word count\").setMaster(\"local[3]\")\n",
    "sc = SparkContext(conf=configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "lines = sc.textFile(\"data/wordcount.txt\")\n",
    "\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Transformasi : Set operations\n",
    "rdd dpt mendukung banyak operasi math, salah satu yg popular adlh sample & distinct\n",
    "\n",
    "## sample\n",
    "create random sample from rdd, purpose for testing\n",
    "\n",
    "menerima 3 param, penjelasannya menit 0:52\n",
    "\n",
    "## distinct \n",
    "sama sprti di sql yaitu data yg unik\n",
    "\n",
    "operasi ini sangat mahal karena hrs shuffing semua data termasuk partisi yg berbeda \n",
    "\n",
    "disarankan untuk tdk memakai operasi ini jika tdk perlu untuk handle duplikat data\n",
    "\n",
    "## set operations\n",
    "berjln pd 2 rdd & menghslkan 1 rdd. \n",
    "\n",
    "yg popular adlh union, intersection, subtract, cartesian product. semuanya adlh rdd dimana 2 rdd tsb memiliki tipe data harus sama. lebih jelas lihat 2:32 - 4:35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case : kita punya 2 data log dari nasa dlm bntk tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union operation\n",
    "# gabungkan 2 file tsb jd 1 rdd, lalu ambil sample sebanyak 0.1 atau 1% dr data baru tsb\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# pertama kita harus hapus dulu header data\n",
    "def isNotHeader(line: str):\n",
    "    return not (line.startswith(\"host\") and \"bytes\" in line)\n",
    "\n",
    "conf = SparkConf().setAppName(\"union Logs\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# rdd 1\n",
    "julyFirstLogs = sc.textFile(\"data/nasa_19950701.tsv\")\n",
    "# rdd 2\n",
    "augustFirstLogs = sc.textFile(\"data/nasa_19950801.tsv\")\n",
    "\n",
    "# gabungkan dg union dimana rdd 2 dimasukkan kedlm rdd 1\n",
    "aggregatedLogLines = julyFirstLogs.union(augustFirstLogs)\n",
    "\n",
    "# hapus header\n",
    "cleanLogLines = aggregatedLogLines.filter(isNotHeader)\n",
    "\n",
    "# ambil data sebanyak 1% as sample\n",
    "sample = cleanLogLines.sample(withReplacement = True, fraction = 0.1)\n",
    "\n",
    "sample.saveAsTextFile(\"data_result/sample_nasa_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lihat part-00xx, dari sana dpt disimpulkan dimana dihslkan 4 file part artinya spark berjln pd 4 worker thread (4 core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 kasus yg sama sprti 10 tp beda cara\n",
    "kali ini kita gunakan intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"sameHosts\").setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "julyFirstLogs = sc.textFile(\"data/nasa_19950701.tsv\")\n",
    "augustFirstLogs = sc.textFile(\"data/nasa_19950801.tsv\")\n",
    "\n",
    "# delimiter dg tab\n",
    "julyFirstHosts = julyFirstLogs.map(lambda line: line.split(\"\\t\")[0])\n",
    "augustFirstHosts = augustFirstLogs.map(lambda line: line.split(\"\\t\")[0])\n",
    "\n",
    "# gabungkan rdd 1 & rrd 2 menggunakan intersection lewat kolom host\n",
    "intersection = julyFirstHosts.intersection(augustFirstHosts)\n",
    "\n",
    "# hapus header (host)\n",
    "cleanedHostIntersection = intersection.filter(lambda host: host != \"host\")\n",
    "cleanedHostIntersection.saveAsTextFile(\"out/nasa_logs_same_hosts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lihat part-00xx, dari sana dpt disimpulkan dimana dihslkan 2 file part artinya spark berjln pd 2 worker thread (2 core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 : action\n",
    "pembahasannya action yg bekerja dlm rdd.\n",
    "\n",
    "mrpkn operasi ke 2 dr rdd stlh transformasi. \n",
    "\n",
    "will return a final value to the driver program or persist data to an external storage system\n",
    "\n",
    "force the evaluation of the transformation required for the rdd yg dipanggil/dijlnkan.\n",
    "\n",
    "## action yg popular digunakan\n",
    "collect, count, countByValue, take, saveAsTextFile, reduce\n",
    "### collect\n",
    "retrieve entire rdd (mengambil semua rdd) lalu return semuanya ke driver program dlm bntk regular collection atau nilai (value). contoh we have string rdd, when we call collect action to string rdd tsb maka we will get a list of strings. hasil tsb dpt kita manipulasi sprti print out atau simpan kedlm suatu file\n",
    "\n",
    "useful when filter dimana memiliki small size & we want deal di local.\n",
    "\n",
    "action collect ini tdk boleh digunakan pd dataset yg besar karena ketika action collect dipanggil/dijlnkan maka semua dataset hrs fit in the memory on single machine untuk dicopy ke driver.\n",
    "\n",
    "seringnya digunakan saat unit test, untuk membandingkan nilai rdd dg nilai ekspektasi dg catatan selama semua data dr rdd can fit in memory.\n",
    "\n",
    "contoh sbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark\n",
      "hadoop\n",
      "spark\n",
      "hive\n",
      "pig\n",
      "cassandra\n",
      "hadoop\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"collect action\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# list of word in the driver program\n",
    "inputWords = [\"spark\",\"hadoop\",'spark','hive','pig','cassandra','hadoop']\n",
    "\n",
    "# convert string word into string rdd\n",
    "wordRdd = sc.parallelize(inputWords)\n",
    "\n",
    "# call action collect to convert back, string rdd to a list of strings sprti awal mulanya\n",
    "words = wordRdd.collect()\n",
    "\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count & countByValue\n",
    "untuk mengetahui jumlah row dlm rdd cara tercepatnya adlh dg mengunakan count\n",
    "\n",
    "countByValue will look nilai yg unik pada setiap row dr rdd & return a map of each unique value to its count. useful when our rdd contain duplicate rows dimana we want know how many of each unique row value we have. contoh kode nya ada di bagian 9\n",
    "\n",
    "contoh count & countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 7\n",
      "count by value :\n",
      "spark : 2\n",
      "hadoop : 2\n",
      "hive : 1\n",
      "pig : 1\n",
      "cassandra : 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"count & count by value\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# list of word in the driver program\n",
    "inputWords = [\"spark\",\"hadoop\",'spark','hive','pig','cassandra','hadoop']\n",
    "\n",
    "# convert string word into string rdd\n",
    "wordRdd = sc.parallelize(inputWords)\n",
    "\n",
    "# count\n",
    "print(\"Count : {}\".format(wordRdd.count()))\n",
    "\n",
    "# countByValue\n",
    "wordCountByValue = wordRdd.countByValue()\n",
    "print(\"count by value :\")\n",
    "for word, count in wordCountByValue.items():\n",
    "    print(\"{} : {}\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take\n",
    "mengambil sejumlah n element dari seluruh data\n",
    "\n",
    "useful when take a peek at the rdd for unit tests & quick debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark\n",
      "hadoop\n",
      "spark\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"take\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# list of word in the driver program\n",
    "inputWords = [\"spark\",\"hadoop\",'spark','hive','pig','cassandra','hadoop']\n",
    "\n",
    "# convert string word into string rdd\n",
    "wordRdd = sc.parallelize(inputWords)\n",
    "\n",
    "# call take, return 3 element\n",
    "words = wordRdd.take(3)\n",
    "\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile\n",
    "meyimpan data (write data out) to a distributed storage system sprti HDFS, amazon S3, simpan di lokal, dll.\n",
    "\n",
    "contoh kode nya dibagian awal sdh diperlihatkan lewat func saveAsTextFile\n",
    "\n",
    "### reduce\n",
    "take a func yg akan melakukan operasi pd 2 element pd input rdd dan return element baru dg tipe yg sama. memakai binary func \n",
    "\n",
    "func ini akan menghslkan hasil yg sama ketika repetitively applied on the same set of rdd data & reduces to a single value\n",
    "\n",
    "dg reduce we can menjlnkan tipe agregasi yg berbeda beda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product : 120\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"reduce\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "inputInt = [1,2,3,4,5]\n",
    "\n",
    "# rdd \n",
    "intRdd = sc.parallelize(inputInt)\n",
    "\n",
    "# lihat 7:01-7:22\n",
    "product = intRdd.reduce(lambda x,y:x*y)\n",
    "print(\"product : {}\".format(product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 case\n",
    "hitung sum of 100 bilangan prima pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum : 24133\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"sum of 100 prime\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# load data in rdd form\n",
    "lines = sc.textFile(\"data/prime.txt\")\n",
    "# split with tab\n",
    "numbers = lines.flatMap(lambda line: line.split(\"\\t\"))\n",
    "\n",
    "# validate data, karena ada kemungkinan terdpt empty string & ini akan return as false\n",
    "validNumb = numbers.filter(lambda number: number)\n",
    "\n",
    "# convert data into int\n",
    "intNumb = validNumb.map(lambda number: int(number))\n",
    "\n",
    "# reduce for sum of the entire number, the lambda func take 2 arguments and return 1 value\n",
    "print(\"sum : {}\".format(intNumb.reduce(lambda x,y: x+y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit namefile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 aspek yg penting dlm rdd\n",
    "rdd bersifat terdistribusi\n",
    "\n",
    "setiap rdd terdiri atas banyak bagian/komponen yg disbt dg partisi(partisions) & partisi ini akan terbagi/dibagi di seluruh cluster. misal cluster terdiri dr 8 node, rdd can split into 8 partisi. jumlah dari partisi is configurable dan rdd tersebar di beberapa node yg dapat dioperasikan di setiap node secara paralel dan independen\n",
    "\n",
    "proses partisi dilakukan scra otomatis by spark\n",
    "\n",
    "## rdd bersifat immutable\n",
    "stlh dibuat datanya tdk dpt diubah/diupdate\n",
    "\n",
    "## rdd bersifat resilient\n",
    "deterministic function of their input. ini adlh nilai + dr immutabilitas & artinya pd bagian rdd dpt recreated at any time\n",
    "\n",
    "spark can recover the parts of the rdd from the input & pick up where it left off\n",
    "\n",
    "rdd are fault tolerant (toleran terhadap kesalahan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 kesimpulan : operasi pd rdd\n",
    "operasi pd rdd hanya 2 yaitu transformasi dan action\n",
    "\n",
    "transformasi : operasi pd rdd yg akan return a new rdd such as map, filter\n",
    "\n",
    "action : operasi yg akan return a result to the driver program or write it to storage and kick off a computation(melakukan proses perhitungan), contoh count, collect, dll\n",
    "\n",
    "ke2nya berbeda karena cara bagaimana spark menghitung rdd\n",
    "\n",
    "meskipun kita dpt define rdd yg baru kapanpun tp rdd hanya dpt dihitung(computed) oleh spark in a lazy fashion, which is the first time rdd are used in an action\n",
    "\n",
    "0:53"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
